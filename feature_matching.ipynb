{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2  \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in images from a filepath as graycsale.\n",
    "rootpath_left='dataset/train/left'\n",
    "rootpath_right='dataset/train/right'\n",
    "gray = cv2.imread(os.path.join(rootpath_left, 'aaz.jpg'))\n",
    "scene_gray = cv2.imread(os.path.join(rootpath_right, 'mqw.jpg'))\n",
    "\n",
    "# Optional: Create a modified image by adding scale invariance and rotation invariance\n",
    "\n",
    "#scene_gray = cv2.pyrDown(gray) #blurs an image and downsamples it\n",
    "#rows, cols = scene_gray.shape[:2] #in case this is not a greyscale image\n",
    "#rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1) #calculate an affine matrix of 2D rotation\n",
    "#scene_gray = cv2.warpAffine(scene_gray, rotation_matrix, (cols, rows)) #apply an affine transformation to image\n",
    "\n",
    "def imshow(image, *args, **kwargs):\n",
    "    if len(image.shape) == 3:\n",
    "      # Height, width, channels\n",
    "      # Assume BGR, do a conversion since \n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "      # Height, width - must be grayscale\n",
    "      # convert to RGB, since matplotlib will plot in a weird colormap (instead of black = 0, white = 1)\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    # Draw the image\n",
    "    plt.imshow(image, *args, **kwargs)\n",
    "    # We'll also disable drawing the axes and tick marks in the plot, since it's actually an image\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display original image and scene image\n",
    "plt.subplots(figsize=(10, 10)) \n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Original Image')\n",
    "imshow(gray)  \n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Match Image')\n",
    "imshow(scene_gray)  \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create() # if cv2 version >= 4.4.0 \n",
    "# sift = cv2.xfeatures2d.SIFT_create() # if cv2 version = 4.3.x \n",
    "\n",
    "# Compute SIFT keypoints and descriptors\n",
    "kp1, des1 = sift.detectAndCompute(gray,None)\n",
    "kp2, des2 = sift.detectAndCompute(scene_gray,None)\n",
    "\n",
    "# Draws the small circles on the locations of keypoints without size\n",
    "kp1_without_size = cv2.drawKeypoints(gray,kp1,None,color = (0, 0, 255)\n",
    "                                     #, color = (0, 0, 255) #If you want a specific colour\n",
    "                                    )\n",
    "kp2_without_size = cv2.drawKeypoints(scene_gray,kp2,None,color = (0, 0, 255)\n",
    "                                     #, color = (0, 0, 255) #If you want a specific colour\n",
    "                                    )\n",
    "\n",
    "# Draws a circle with the size of each keypoint and show its orientation\n",
    "kp1_with_size = cv2.drawKeypoints(gray,kp1,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "kp2_with_size = cv2.drawKeypoints(scene_gray,kp2,None,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "\n",
    "# Display images with&without the size of keypoints \n",
    "plt.subplots(figsize=(15, 10)) \n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "imshow(kp1_without_size)  \n",
    "plt.title('Original Image keypoints without size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "imshow(kp2_without_size)  \n",
    "plt.title('Scene Image keypoints without size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "imshow(kp1_with_size)  \n",
    "plt.title('Original Image keypoints with size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "imshow(kp2_with_size)  \n",
    "plt.title('Scene Image keypoints with size')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show() \n",
    "\n",
    "# Print the number of keypoints detected\n",
    "print(\"Number of keypoints detected in the original image: \", len(kp1))\n",
    "print(\"Number of keypoints detected in the Scene image: \", len(kp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN parameters and initialize\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "# Matching descriptor using KNN algorithm\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "# Create a mask to draw all good matches\n",
    "matchesMask = []\n",
    "\n",
    "# Store all good matches as per Lowe's Ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "        matchesMask.append([1,0]) # Match\n",
    "    else:\n",
    "        matchesMask.append([0,0]) # Mismatch\n",
    "       \n",
    "        \n",
    "# Draw all good matches\n",
    "draw_params = dict(#matchColor = (0,255,0),  #If you want a specific colour\n",
    "                   #singlePointColor = (255,0,0), #If you want a specific colour\n",
    "                    matchesMask = matchesMask,\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "\n",
    "good_matches = cv2.drawMatchesKnn(gray,kp1,scene_gray,kp2,matches,None,**draw_params)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "imshow(good_matches)\n",
    "plt.title('All good matches')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print total number of good matches between two images\n",
    "print(\"\\nNumber of good matches between two images: \", len(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we set a condition that at least N matches (defined by MIN_MATCH_NUM) are required to find the object. \n",
    "MIN_MATCH_NUM = 4\n",
    "\n",
    "if len(good)>= MIN_MATCH_NUM:\n",
    "    # If enough matches are found, we extract the positions of the matched keypoints in both images. \n",
    "    # They are passed to find the perspective transformation. \n",
    "    \n",
    "    # Estimate homography between two images\n",
    "    ptsA = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    ptsB = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)\n",
    "    H, status = cv2.findHomography(ptsA, \n",
    "                                   ptsB, \n",
    "                                   cv2.RANSAC, \n",
    "                                   ransacReprojThreshold = 5, \n",
    "                                   maxIters = 10) # try to change maxIters and see the effect\n",
    "    # Where H is the resulting single-strain matrix.\n",
    "    # status returns a list of feature points that represent successful matches.\n",
    "    # ptsA, ptsB are keypoints.\n",
    "    # The three parameters cv2.RANSAC, ransacReprojThreshold, maxIters are related to RANSAC.\n",
    "    # ransacReprojThreshold: Maximum reprojection error in the RANSAC algorithm to consider a point as an inlier. \n",
    "    # maxIters: The maximum number of RANSAC-based robust method iterations.\n",
    "    \n",
    "    success = status.ravel().tolist()\n",
    "    \n",
    "    # Draw detected template in scene image\n",
    "    imgOut = cv2.warpPerspective(scene_gray, H, (gray.shape[1],gray.shape[0]),\n",
    "                             flags=cv2.INTER_LINEAR + cv2.WARP_INVERSE_MAP)\n",
    "    \n",
    "    # Print total number of successful matches between two images\n",
    "    print(\"\\nNumber of successful matches between two images: \", success.count(1)) # Returns the number of 1 in the success list\n",
    "\n",
    "else:\n",
    "    # Otherwise, print that “Not enough matches are found”.\n",
    "    print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_NUM) )\n",
    "    success = None\n",
    "\n",
    "\n",
    "# Draw our inliers (if successfully found the object) or all matching keypoints (if failed)\n",
    "draw_params = dict(#matchColor = (0,255,0), # draw in a specific colour\n",
    "                   #singlePointColor = (255,0,0), # draw in a specific colour\n",
    "                   matchesMask = success, # draw only inliers\n",
    "                   flags = 2)\n",
    "\n",
    "success_matches = cv2.drawMatches(gray,kp1,scene_gray,kp2,good,None,**draw_params)\n",
    "\n",
    "\n",
    "# Plotting results\n",
    "plt.subplots(figsize=(9, 9)) \n",
    "\n",
    "if success == None:\n",
    "    imshow(success_matches)\n",
    "    plt.title('All matching keypoints')\n",
    "    plt.axis('off')\n",
    "    \n",
    "else:\n",
    "    plt.subplot(2,1,1)\n",
    "    imshow(success_matches)\n",
    "    plt.title('All successful matches')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    imshow(imgOut, 'gray')\n",
    "    plt.title('Display detected template in scene image')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
